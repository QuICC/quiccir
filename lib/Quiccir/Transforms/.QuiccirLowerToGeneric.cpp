//====- QuiccirLowerToGeneric.cpp - Lowering from Quiccir ops to Generic ops --===//
//
// This file implements a partial lowering of Quiccir operations to Generic
// operations (quadrature or FFT based ops)
//
//===----------------------------------------------------------------------===//

#include "Quiccir/Transforms/QuiccirPassDetail.h"

#include "Quiccir/IR/QuiccirDialect.h"
#include "Quiccir/IR/QuiccirOps.h"
#include "Quiccir/Transforms/QuiccirPasses.h"
#include "Quiccir/Transforms/TypeConverter.hpp"

#include "mlir/IR/BuiltinDialect.h"
#include "mlir/Dialect/Arith/IR/Arith.h"
#include "mlir/Dialect/Bufferization/IR/BufferizableOpInterface.h"
#include "mlir/Dialect/Bufferization/IR/Bufferization.h"
#include "mlir/Dialect/Bufferization/Transforms/Bufferize.h"
#include "mlir/Dialect/Func/IR/FuncOps.h"
#include "mlir/Dialect/LLVMIR/LLVMDialect.h"
#include "mlir/Dialect/MemRef/IR/MemRef.h"
#include "mlir/Pass/Pass.h"
#include "mlir/Transforms/DialectConversion.h"
#include "llvm/ADT/Sequence.h"

using namespace mlir;
using namespace mlir::quiccir;


static MemRefType makeStridedLayoutDynamic(MemRefType type) {
  return MemRefType::Builder(type).setLayout(StridedLayoutAttr::get(
      type.getContext(), ShapedType::kDynamic,
      SmallVector<int64_t>(type.getRank(), ShapedType::kDynamic)));
}

/// Helper function to extract the operand types that are passed to the
/// generated CallOp. MemRefTypes have their layout canonicalized since the
/// information is not used in signature generation.
/// Note that static size information is not modified.
static SmallVector<Type, 4> extractOperandTypes(SmallVector<Type, 4>& types) {
  SmallVector<Type, 4> result;
  for (auto type : types) {
    // The underlying descriptor type (e.g. LLVM) does not have layout
    // information. Canonicalizing the type at the level of std when going into
    // a library call avoids needing to introduce DialectCastOp.
    if (auto memrefType = type.dyn_cast<MemRefType>())
      result.push_back(makeStridedLayoutDynamic(memrefType));
    else
      result.push_back(type);
  }
  return result;
}

// Get a SymbolRefAttr containing the library function name for the op.
// If the library function does not exist, insert a declaration.
template <class OpT>
static FailureOr<FlatSymbolRefAttr>
getLibraryCallSymbolRef(Operation *op, PatternRewriter &rewriter, SmallVector<Type, 4>& types) {
  auto implOp = cast<OpT>(op);
  std::string fnName = implOp.getOperationName().str();
  std::replace(fnName.begin(), fnName.end(), '.', '_');
  if (fnName.empty())
    return rewriter.notifyMatchFailure(op, "No library call defined for: ");

  // fnName is a dynamic std::string, unique it via a SymbolRefAttr.
  FlatSymbolRefAttr fnNameAttr =
      SymbolRefAttr::get(rewriter.getContext(), fnName);
  auto module = op->getParentOfType<ModuleOp>();
  if (module.lookupSymbol(fnNameAttr.getAttr()))
    return fnNameAttr;

  SmallVector<Type, 4> inputTypes(extractOperandTypes(types));
  auto libFnType = rewriter.getFunctionType(inputTypes, {});

  OpBuilder::InsertionGuard guard(rewriter);
  // Insert before module terminator.
  rewriter.setInsertionPoint(module.getBody(),
                             std::prev(module.getBody()->end()));
  func::FuncOp funcOp = rewriter.create<func::FuncOp>(
      op->getLoc(), fnNameAttr.getValue(), libFnType);
  // Insert a function attribute that will trigger the emission of the
  // corresponding `_mlir_ciface_xxx` interface so that external libraries see
  // a normalized ABI. This interface is added during std to llvm conversion.
  funcOp->setAttr(LLVM::LLVMDialect::getEmitCWrapperAttrName(),
                  UnitAttr::get(op->getContext()));
  funcOp.setPrivate();
  return fnNameAttr;
}


static SmallVector<Value, 4>
createTypeCanonicalizedMemRefOperands(OpBuilder &b, Location loc,
                                      ValueRange operands) {
  SmallVector<Value, 4> res;
  res.reserve(operands.size());
  for (auto op : operands) {
    auto memrefType = dyn_cast<MemRefType>(op.getType());
    if (!memrefType) {
      res.push_back(op);
      continue;
    }
    Value cast =
        b.create<memref::CastOp>(loc, makeStridedLayoutDynamic(memrefType), op);
    res.push_back(cast);
  }
  return res;
}


//===----------------------------------------------------------------------===//
// QuiccirToStd RewritePatterns
//===----------------------------------------------------------------------===//

/// Insert an allocation for the given MemRefType.
/// Dealloc should be generated by a pass a posteriori
static Value insertAlloc(Type type, Location loc,
                                   SmallVector<Value, 4> dynamicOperands,
                                   PatternRewriter &rewriter) {
  // Alloc
  auto alloc = rewriter.create<memref::AllocOp>(loc, type, dynamicOperands);

  // Make sure to allocate at the beginning of the block.
  auto *parentBlock = alloc->getBlock();
  alloc->moveBefore(&parentBlock->front());

  return alloc;
}

namespace {

//===----------------------------------------------------------------------===//
// QuiccirToStd RewritePatterns: Quadrature operations
//===----------------------------------------------------------------------===//

struct QuadratureOpLowering : public ConversionPattern {
  QuadratureOpLowering(MLIRContext *ctx, TypeConverter &typeConverter)
      : ConversionPattern(typeConverter, quiccir::QuadratureOp::getOperationName(), /*benefit=*/ 1, ctx) {}

  LogicalResult
  matchAndRewrite(Operation *op, ArrayRef<Value> operands,
                  ConversionPatternRewriter &rewriter) const final {


    // operands are converted by the TypeConverter
    quiccir::QuadratureOpAdaptor quadAdaptor(operands);
    Value opBuffer = quadAdaptor.getOprtr();

    Value umodBuffer = quadAdaptor.getUmods();

    auto loc = op->getLoc();

    // Insert an allocation and deallocation for the result of this operation.
    auto retTensorType = (*op->result_type_begin()).cast<TensorType>();
    auto retMemRefType = getTypeConverter()->convertType(retTensorType);

    // Check dynamic shape dimensions
    /// \todo not supported correctly
    SmallVector<Value, 4> dynamicOperands;
    // for (int i = 0; i < retMemRefType.getRank(); ++i) {
    //   if (!retMemRefType.isDynamicDim(i))
    //     continue;
    //   Value size = rewriter.createOrFold<arith::ConstantIndexOp>(loc, i);
    //   /// \todo this is not right, the size should be the result size of the batched matmul
    //   Value dim =
    //       rewriter.createOrFold<memref::DimOp>(loc, umodBuffer, size);
    //   dynamicOperands.push_back(dim);
    // }

    auto retAlloc = insertAlloc(retMemRefType, loc, dynamicOperands, rewriter);

    SmallVector <Type, 4> typeOperands = {retMemRefType, opBuffer.getType(), umodBuffer.getType()};

    // return val becomes first arg
    auto libraryCallName = getLibraryCallSymbolRef<QuadratureOp>(op, rewriter, typeOperands);
    if (failed(libraryCallName))
      return failure();

    SmallVector<Value, 4> newOperands = {retAlloc, opBuffer, umodBuffer};
    rewriter.create<func::CallOp>(
        loc, libraryCallName->getValue(), TypeRange(),
        createTypeCanonicalizedMemRefOperands(rewriter, loc,
                                              newOperands));

    // rewriter.replaceOp(op, newOp);
    rewriter.replaceOpWithNewOp<bufferization::ToTensorOp>(op, retAlloc);

    return success();
  }
};

} // namespace

//===----------------------------------------------------------------------===//
// QuiccirToCallLoweringPass
//===----------------------------------------------------------------------===//

/// This is a partial lowering to affine loops of the quiccir operations
namespace {
struct QuiccirToCallLoweringPass
    : public QuiccirLowerToCallBase<QuiccirToCallLoweringPass> {
  void runOnOperation() final;
};
} // namespace

void QuiccirToCallLoweringPass::runOnOperation() {
  // The first thing to define is the conversion target. This will define the
  // final target for this lowering.
  ConversionTarget target(getContext());

  // Type converters
  mlir::bufferization::BufferizeTypeConverter buffConverter;
  quiccir::TensorToViewConverter viewConverter;

  // We define the specific operations, or dialects, that are legal targets for
  // this lowering. In our case, we are lowering to a combination of the
  // `Affine`, `Arithmetic`, `Func`, and `MemRef` dialects.
  target
      .addLegalDialect<BuiltinDialect, arith::ArithDialect,
                       func::FuncDialect, memref::MemRefDialect,
                       bufferization::BufferizationDialect,
                       LLVM::LLVMDialect>();

  // We also define the Quiccir dialect as Illegal so that the conversion will fail
  // if any of these operations are *not* converted.
  target.addIllegalDialect<quiccir::QuiccirDialect>();
  // Except that we need alloc / materialize to be legal
  target.addLegalOp<quiccir::AllocOp, quiccir::MaterializeOp>();

  // Now that the conversion target has been defined, we just need to provide
  // the set of patterns that will lower the Quiccir operations.
  RewritePatternSet patterns(&getContext());
  patterns.add<QuadratureOpLowering>(
      &getContext(), buffConverter);

  // void populateViewConversionPatterns(TypeConverter &typeConverter,
  // RewritePatternSet &patterns)
  // patterns.add<ViewTypeToPtrOfStructConverter>(converter, &getContext());

  // With the target and rewrite patterns defined, we can now attempt the
  // conversion. The conversion will signal failure if any of our `illegal`
  // operations were not converted successfully.
  if (failed(
          applyPartialConversion(getOperation(), target, std::move(patterns))))
    signalPassFailure();
}

/// Create a pass for lowering operations to library calls
std::unique_ptr<Pass> mlir::quiccir::createLowerToCallPass() {
  return std::make_unique<QuiccirToCallLoweringPass>();
}
